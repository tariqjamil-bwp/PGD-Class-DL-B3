{"cells":[{"cell_type":"markdown","metadata":{"id":"q8W78Vr-7fUr"},"source":["This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n","\n","**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n","\n","This notebook was generated for TensorFlow 2.6."]},{"cell_type":"markdown","metadata":{"id":"4y4FTin-7fUu"},"source":["## Beyond text classification: Sequence-to-sequence learning"]},{"cell_type":"markdown","metadata":{"id":"m7VfwfSK7fUu"},"source":["### A machine translation example"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1039,"status":"ok","timestamp":1700149478822,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"FPHXDZfU7fUu","outputId":"d8f69a6c-b5e3-4115-e8c9-19bbaa7f9c58"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'rf': No such file or directory\n","--2023-11-16 15:44:37--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.161.207, 74.125.126.207, 142.251.172.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.161.207|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2638744 (2.5M) [application/zip]\n","Saving to: ‘spa-eng.zip’\n","\n","spa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.008s  \n","\n","2023-11-16 15:44:37 (308 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n","\n"]}],"source":["!rm rf spa-eng*\n","!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","\n","!unzip -q spa-eng.zip"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":649,"status":"ok","timestamp":1700149479910,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"FjyV8X7Y7fUv"},"outputs":[],"source":["text_file = \"spa-eng/spa.txt\"\n","with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    english, spanish = line.split(\"\\t\")\n","    spanish = \"[start] \" + spanish + \" [end]\"\n","    text_pairs.append((english, spanish))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1700149479911,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"tqP5ylj47fUv","outputId":"6d8335eb-38fd-406b-885e-9a16dd40853d"},"outputs":[{"output_type":"stream","name":"stdout","text":["('I felt kind of sorry for Tom.', '[start] Me sentí un poco mal por Tom. [end]')\n"]}],"source":["import random\n","print(random.choice(text_pairs))"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1700149479911,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"u1qowz2q7fUv"},"outputs":[],"source":["import random\n","random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]"]},{"cell_type":"markdown","metadata":{"id":"f-l36HMT7fUv"},"source":["**Vectorizing the English and Spanish text pairs**"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":24371,"status":"ok","timestamp":1700149504275,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"ZYqtVSVx7fUv"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","import string\n","import re\n","\n","strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_spanish_texts = [pair[1] for pair in train_pairs]\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"]},{"cell_type":"markdown","metadata":{"id":"7VGkwjq47fU2"},"source":["**Preparing datasets for the translation task**"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1307,"status":"ok","timestamp":1700149505540,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"MnhcL2a_7fU3"},"outputs":[],"source":["import tensorflow as tf\n","\n","batch_size = 64\n","\n","def format_dataset(eng, spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\n","        \"english\": eng,\n","        \"spanish\": spa[:, :-1],\n","    }, spa[:, 1:])\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1700149505540,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"qq79ixZt7fU3","outputId":"2f33f9cb-de87-4133-e77f-a5fa29efc0a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (64, 20)\n","inputs['spanish'].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}],"source":["for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"7NKlz_oe7fU3"},"source":["### Sequence-to-sequence learning with RNNs"]},{"cell_type":"markdown","metadata":{"id":"TWoI0TBP7fU3"},"source":["**GRU-based encoder**"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4404,"status":"ok","timestamp":1700149509936,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"cgSitvTu7fU3"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","embed_dim = 256\n","latent_dim = 1024\n","\n","source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n","encoded_source = layers.Bidirectional(\n","    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"]},{"cell_type":"markdown","metadata":{"id":"QxYM4HwZ7fU3"},"source":["**GRU-based decoder and the end-to-end model**"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":921,"status":"ok","timestamp":1700149510840,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"TklMt_zo7fU4"},"outputs":[],"source":["past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n","decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n","x = decoder_gru(x, initial_state=encoded_source)\n","x = layers.Dropout(0.5)(x)\n","target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","seq2seq_rnn = keras.Model([source, past_target], target_next_step)"]},{"cell_type":"markdown","metadata":{"id":"Eqj5AjWq7fU4"},"source":["**Training our recurrent sequence-to-sequence model**"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1811156,"status":"ok","timestamp":1700151321992,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"ogeSKTW77fU4","outputId":"71b99921-b2c9-4e1d-a04d-29e98a4fa7a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","1302/1302 [==============================] - 153s 101ms/step - loss: 4.6916 - accuracy: 0.3179 - val_loss: 3.9194 - val_accuracy: 0.3855\n","Epoch 2/15\n","1302/1302 [==============================] - 111s 85ms/step - loss: 3.7285 - accuracy: 0.4147 - val_loss: 3.2607 - val_accuracy: 0.4667\n","Epoch 3/15\n","1302/1302 [==============================] - 122s 94ms/step - loss: 3.2154 - accuracy: 0.4721 - val_loss: 2.8795 - val_accuracy: 0.5160\n","Epoch 4/15\n","1302/1302 [==============================] - 121s 93ms/step - loss: 2.8559 - accuracy: 0.5131 - val_loss: 2.6285 - val_accuracy: 0.5512\n","Epoch 5/15\n","1302/1302 [==============================] - 111s 85ms/step - loss: 2.5810 - accuracy: 0.5461 - val_loss: 2.4528 - val_accuracy: 0.5769\n","Epoch 6/15\n","1302/1302 [==============================] - 122s 93ms/step - loss: 2.3582 - accuracy: 0.5744 - val_loss: 2.3226 - val_accuracy: 0.5974\n","Epoch 7/15\n","1302/1302 [==============================] - 122s 94ms/step - loss: 2.1729 - accuracy: 0.5988 - val_loss: 2.2280 - val_accuracy: 0.6124\n","Epoch 8/15\n","1302/1302 [==============================] - 111s 86ms/step - loss: 2.0132 - accuracy: 0.6199 - val_loss: 2.1570 - val_accuracy: 0.6222\n","Epoch 9/15\n","1302/1302 [==============================] - 111s 85ms/step - loss: 1.8763 - accuracy: 0.6388 - val_loss: 2.0956 - val_accuracy: 0.6320\n","Epoch 10/15\n","1302/1302 [==============================] - 111s 85ms/step - loss: 1.7555 - accuracy: 0.6553 - val_loss: 2.0577 - val_accuracy: 0.6388\n","Epoch 11/15\n","1302/1302 [==============================] - 111s 85ms/step - loss: 1.6517 - accuracy: 0.6696 - val_loss: 2.0238 - val_accuracy: 0.6437\n","Epoch 12/15\n","1302/1302 [==============================] - 110s 85ms/step - loss: 1.5575 - accuracy: 0.6834 - val_loss: 1.9997 - val_accuracy: 0.6491\n","Epoch 13/15\n","1302/1302 [==============================] - 110s 85ms/step - loss: 1.4752 - accuracy: 0.6949 - val_loss: 1.9777 - val_accuracy: 0.6534\n","Epoch 14/15\n","1302/1302 [==============================] - 110s 85ms/step - loss: 1.4009 - accuracy: 0.7058 - val_loss: 1.9592 - val_accuracy: 0.6546\n","Epoch 15/15\n","1302/1302 [==============================] - 110s 85ms/step - loss: 1.3393 - accuracy: 0.7154 - val_loss: 1.9409 - val_accuracy: 0.6590\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x79852c32f400>"]},"metadata":{},"execution_count":12}],"source":["seq2seq_rnn.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"]},{"cell_type":"markdown","metadata":{"id":"93Qnhp1L7fU4"},"source":["**Translating new sentences with our RNN encoder and decoder**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16194,"status":"ok","timestamp":1700151338172,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"81x4g6os7fU4","outputId":"e5edeccb-35e8-430d-f2a4-ea54a49a9662"},"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","He condemned those who opposed his policies.\n","1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","[start] Él [UNK] a su familia que los [UNK] [end]\n","-\n","I'll see if Tom's here.\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","[start] voy a ver a tom aquí [end]\n","-\n","I already told you he isn't here.\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","[start] te dije que él no está aquí [end]\n","-\n","Your fly is open!\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","[start] tu [UNK] está cerrada [end]\n","-\n","They contributed money to the Red Cross.\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 37ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","[start] ellos [UNK] dinero para la cruz roja [end]\n","-\n","How're you doing?\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 34ms/step\n","[start] qué te puedes hacer [end]\n","-\n","I asked Tom to help me pull weeds in the garden.\n","1/1 [==============================] - 0s 41ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 46ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 42ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 36ms/step\n","1/1 [==============================] - 0s 33ms/step\n","[start] le pedí a tom que me [UNK] a [UNK] en el jardín [end]\n","-\n","On hearing the news, she turned pale.\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 39ms/step\n","1/1 [==============================] - 0s 33ms/step\n","[start] al oír la noticia se hizo un fuerte [end]\n","-\n","I don't think you can beat me.\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 34ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 38ms/step\n","1/1 [==============================] - 0s 25ms/step\n","[start] no creo que me puede [UNK] [end]\n","-\n","The plane arrived at New York on schedule.\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","[start] el avión llegó a nueva york en nueva [end]\n","-\n","Get ready for some action.\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","[start] [UNK] por una cuenta de [UNK] [end]\n","-\n","I don't want you to see me naked.\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","[start] no quiero que te [UNK] [end]\n","-\n","Tom showed Mary a picture of John and another boy.\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 27ms/step\n","[start] tom le enseñó a mary una foto de su hijo y john [end]\n","-\n","Do whatever needs to be done.\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","[start] haz lo que necesita hacer [end]\n","-\n","Why can't Tom come?\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 24ms/step\n","[start] por qué no puede venir tom [end]\n","-\n","These are decisions I want to make alone.\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 26ms/step\n","[start] estos son [UNK] solo quiero hacer solo [end]\n","-\n","They buried him in the graveyard by the church.\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 23ms/step\n","[start] lo [UNK] en la [UNK] de la iglesia [end]\n","-\n","Our fate is in your hands.\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 22ms/step\n","[start] nuestro destino está en las manos [end]\n","-\n","How long will you be away?\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","[start] por cuánto tiempo te [UNK] [end]\n","-\n","I like to play soccer.\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 25ms/step\n","[start] me gusta jugar al fútbol [end]\n"]}],"source":["import numpy as np\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])\n","        next_token_predictions = seq2seq_rnn.predict(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{"id":"QR9OanRQ7fU5"},"source":["### Sequence-to-sequence learning with Transformer"]},{"cell_type":"markdown","metadata":{"id":"DkvNmB2h7fU5"},"source":["#### The Transformer decoder"]},{"cell_type":"markdown","metadata":{"id":"EzJZsJOZ7fU5"},"source":["**The `TransformerDecoder`**"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1700151338172,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"RX0bQfXX7fU5"},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"]},{"cell_type":"markdown","metadata":{"id":"cMgm37o67fU5"},"source":["#### Putting it all together: A Transformer for machine translation"]},{"cell_type":"markdown","metadata":{"id":"7-u_5bYq7fU5"},"source":["**PositionalEmbedding layer**"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700151338173,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"dxqCrvNL7fU5"},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1700151338173,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"OOcJBIHEZGvB"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"5S4u6_XU7fU5"},"source":["**End-to-end Transformer**"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":518,"status":"ok","timestamp":1700151338674,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"},"user_tz":-300},"id":"sHK8eJ7B7fU6"},"outputs":[],"source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"markdown","metadata":{"id":"XwHAI37k7fU6"},"source":["**Training the sequence-to-sequence Transformer**"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yfOJ_AoN7fU6","executionInfo":{"status":"ok","timestamp":1700154357818,"user_tz":-300,"elapsed":3019152,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"}},"outputId":"5e66ffd0-84fa-49ea-d932-8ba3e69bdcfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","1302/1302 [==============================] - 104s 75ms/step - loss: 3.7967 - accuracy: 0.4399 - val_loss: 2.8865 - val_accuracy: 0.5371\n","Epoch 2/30\n","1302/1302 [==============================] - 89s 69ms/step - loss: 2.8512 - accuracy: 0.5500 - val_loss: 2.5267 - val_accuracy: 0.5896\n","Epoch 3/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 2.5555 - accuracy: 0.5934 - val_loss: 2.3992 - val_accuracy: 0.6123\n","Epoch 4/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 2.3900 - accuracy: 0.6207 - val_loss: 2.3560 - val_accuracy: 0.6246\n","Epoch 5/30\n","1302/1302 [==============================] - 91s 70ms/step - loss: 2.2838 - accuracy: 0.6389 - val_loss: 2.3234 - val_accuracy: 0.6324\n","Epoch 6/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 2.2118 - accuracy: 0.6519 - val_loss: 2.3079 - val_accuracy: 0.6378\n","Epoch 7/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 2.1448 - accuracy: 0.6655 - val_loss: 2.2852 - val_accuracy: 0.6474\n","Epoch 8/30\n","1302/1302 [==============================] - 91s 70ms/step - loss: 2.0798 - accuracy: 0.6775 - val_loss: 2.2449 - val_accuracy: 0.6524\n","Epoch 9/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 2.0227 - accuracy: 0.6885 - val_loss: 2.2316 - val_accuracy: 0.6605\n","Epoch 10/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.9782 - accuracy: 0.6961 - val_loss: 2.2468 - val_accuracy: 0.6617\n","Epoch 11/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.9370 - accuracy: 0.7037 - val_loss: 2.2317 - val_accuracy: 0.6611\n","Epoch 12/30\n","1302/1302 [==============================] - 89s 69ms/step - loss: 1.9106 - accuracy: 0.7094 - val_loss: 2.2546 - val_accuracy: 0.6632\n","Epoch 13/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.8844 - accuracy: 0.7143 - val_loss: 2.2547 - val_accuracy: 0.6663\n","Epoch 14/30\n","1302/1302 [==============================] - 89s 69ms/step - loss: 1.8615 - accuracy: 0.7185 - val_loss: 2.2753 - val_accuracy: 0.6668\n","Epoch 15/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.8379 - accuracy: 0.7226 - val_loss: 2.2652 - val_accuracy: 0.6672\n","Epoch 16/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.8213 - accuracy: 0.7257 - val_loss: 2.3097 - val_accuracy: 0.6662\n","Epoch 17/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.8016 - accuracy: 0.7292 - val_loss: 2.3148 - val_accuracy: 0.6662\n","Epoch 18/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.7847 - accuracy: 0.7330 - val_loss: 2.3113 - val_accuracy: 0.6670\n","Epoch 19/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.7716 - accuracy: 0.7350 - val_loss: 2.3354 - val_accuracy: 0.6677\n","Epoch 20/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.7545 - accuracy: 0.7383 - val_loss: 2.3754 - val_accuracy: 0.6637\n","Epoch 21/30\n","1302/1302 [==============================] - 89s 69ms/step - loss: 1.7381 - accuracy: 0.7413 - val_loss: 2.3586 - val_accuracy: 0.6685\n","Epoch 22/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.7261 - accuracy: 0.7435 - val_loss: 2.3727 - val_accuracy: 0.6671\n","Epoch 23/30\n","1302/1302 [==============================] - 89s 69ms/step - loss: 1.7119 - accuracy: 0.7463 - val_loss: 2.3663 - val_accuracy: 0.6714\n","Epoch 24/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.7008 - accuracy: 0.7484 - val_loss: 2.3754 - val_accuracy: 0.6699\n","Epoch 25/30\n","1302/1302 [==============================] - 89s 69ms/step - loss: 1.6871 - accuracy: 0.7509 - val_loss: 2.4095 - val_accuracy: 0.6685\n","Epoch 26/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.6730 - accuracy: 0.7528 - val_loss: 2.3872 - val_accuracy: 0.6715\n","Epoch 27/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.6588 - accuracy: 0.7556 - val_loss: 2.4048 - val_accuracy: 0.6695\n","Epoch 28/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.6479 - accuracy: 0.7577 - val_loss: 2.4201 - val_accuracy: 0.6729\n","Epoch 29/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.6342 - accuracy: 0.7602 - val_loss: 2.4459 - val_accuracy: 0.6702\n","Epoch 30/30\n","1302/1302 [==============================] - 90s 69ms/step - loss: 1.6193 - accuracy: 0.7625 - val_loss: 2.4832 - val_accuracy: 0.6703\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x798525536ec0>"]},"metadata":{},"execution_count":18}],"source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","transformer.fit(train_ds, epochs=30, validation_data=val_ds)"]},{"cell_type":"markdown","metadata":{"id":"TA8RKL727fU6"},"source":["**Translating new sentences with our Transformer model**"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjuDhXhu7fU6","executionInfo":{"status":"ok","timestamp":1700154365395,"user_tz":-300,"elapsed":7582,"user":{"displayName":"TARIQ JAMIL","userId":"15811585553781902842"}},"outputId":"40aed19b-164c-4eac-a020-da45702aa669"},"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","Normally, we don't do that in Spain.\n","[start] normalmente no hagas en londres [end]\n","-\n","I want to leave early.\n","[start] quiero irme temprano [end]\n","-\n","Why is that good?\n","[start] por qué está bien [end]\n","-\n","I'll never see her again.\n","[start] no voy a volver a ver nunca [end]\n","-\n","I prefer working to doing nothing.\n","[start] prefiero trabajar en hacer no [end]\n","-\n","You overestimate him.\n","[start] le [UNK] [end]\n","-\n","You are always complaining about your husband.\n","[start] siempre te estás quejando de tu marido [end]\n","-\n","Please wake me up at six tomorrow.\n","[start] por favor mañana me levanto a la mañana [end]\n","-\n","I'm feeling pretty confident.\n","[start] estoy se me siento bastante bien que te está bien [end]\n","-\n","Keep notes.\n","[start] toma errores [end]\n","-\n","The weather was getting worse and worse.\n","[start] el tiempo va a hacer peor y el clima [end]\n","-\n","I'm warm.\n","[start] soy caliente [end]\n","-\n","I can't follow you.\n","[start] no puedo seguir [end]\n","-\n","I'm a college student.\n","[start] soy estudiante a la misma abuela [end]\n","-\n","Tom explained the rules of the game to Mary.\n","[start] tom explicó las reglas del juego del juego a mary [end]\n","-\n","I think you could be happy here.\n","[start] creo que ustedes podría estar feliz aquí [end]\n","-\n","Everybody was interested in the story.\n","[start] todos estaban interesado en la historia [end]\n","-\n","He is at home.\n","[start] Él está en casa [end]\n","-\n","I'm not one of your employees.\n","[start] no soy uno de tus empleados [end]\n","-\n","You don't have to go to the party if you don't want to.\n","[start] no tienes para ir a la fiesta si no quieres [end]\n"]}],"source":["import numpy as np\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{"id":"fDgkO37O7fU6"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"aH67nybazZvR"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}