{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Packages & Liberaries**\n",
        "### *1a. Import of Packages*"
      ],
      "metadata": {
        "id": "qQGgSiQ4mlFf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1_ZV10ml68e",
        "outputId": "451f6b35-7bb6-4a54-d3c5-bd47bd9a685e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *1b. Import of Packages*"
      ],
      "metadata": {
        "id": "SryqQp38mupa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pGEYi2Oyl0uM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Model Definitions**\n",
        "### *2a. Model Selection & some initializations*"
      ],
      "metadata": {
        "id": "5262gWFmm05B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9AVwm2mol0uO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abe43e3-4cb1-492d-8c12-40f2d8497dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16 cuda:0\n"
          ]
        }
      ],
      "source": [
        "base_model_id = \"microsoft/phi-1_5\"\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] == 8 else compute_dtype\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(dtype, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *2b. Model Configuration Settings*"
      ],
      "metadata": {
        "id": "T63yeudGnp7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Iq26iZCMl0uO"
      },
      "outputs": [],
      "source": [
        "#for better GPU memory management\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *2b. Model & tokenizer Instantiation*"
      ],
      "metadata": {
        "id": "oiKi4imEoqG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "udnrxCVql0uO"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          base_model_id,\n",
        "          trust_remote_code=True,\n",
        "          #quantization_config=bnb_config,     # can be replaced with \"load_in_8bit=True\" # for better response but more VRAM req\n",
        "          torch_dtype = dtype,\n",
        "          device_map={\"\": 0})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "#tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *2c. Checking model / tokenizer loading"
      ],
      "metadata": {
        "id": "PnYyqw1CtrAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from transformers import TextStreamer\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "query = \"What is Gradient Descent?\"\n",
        "\n",
        "inputs = tokenizer(query, return_tensors='pt').to(model.device)\n",
        "output = model.generate(**inputs, streamer=streamer, use_cache=True, max_new_tokens=200, do_sample=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qXyO57Ht0D-",
        "outputId": "fec853f9-6c17-48e1-89d8-7be2a4d80ca9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: Gradient Descent is a mathematical method used to find the lowest possible value for a function.\n",
            "\n",
            "Exercise 5:\n",
            "What is an Algorithm?\n",
            "Answer: An algorithm is a step-by-step process used to solve a problem or complete a task.\n",
            "\n",
            "\n",
            "\n",
            "Logical Reasoning Exercise:\n",
            "\n",
            "Imagine a world where the rules of communication are different. In this world, people communicate through a complex system of sounds and symbols, similar to our language. However, instead of using spoken words, these sounds and symbols are represented using the tones of math. To understand this concept, let's relate it to the foundation of mathematics and the concept of decimals.\n",
            "\n",
            "In mathematics, decimals are a way to express numbers that are less than a whole. They are often used when we need to represent a part of a whole. Decimal numbers are represented using a decimal point, which separates the whole part from the fractional part.\n",
            "CPU times: user 8.21 s, sys: 48.9 ms, total: 8.25 s\n",
            "Wall time: 8.38 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Inference Pipelines**\n",
        "### *3a. Setting up Generation Config*"
      ],
      "metadata": {
        "id": "6LAGcpC0usIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.generation.utils import StoppingCriteria, List, StoppingCriteriaList\n",
        "\n",
        "class StopGenerationCriteria(StoppingCriteria):\n",
        "    def __init__(self, tokens: List[List[str]], tokenizer: AutoTokenizer, device: torch.device):\n",
        "\n",
        "        stop_token_ids = [tokenizer.convert_tokens_to_ids(t) for t in tokens]\n",
        "        self.stop_token_ids = [\n",
        "            torch.tensor(x, dtype=torch.long, device=device) for x in stop_token_ids]\n",
        "\n",
        "    def __call__(\n",
        "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in self.stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids) :], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stop_tokens = [[\"### Human\", \":\"], [\"### Assitant\", \":\"]]\n",
        "stopping_criteria = StoppingCriteriaList(\n",
        "    [StopGenerationCriteria(stop_tokens, tokenizer, model.device)])\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tokenizer, skip_prompt=True, skip_special_tokens=True, use_multiprocessing=False)"
      ],
      "metadata": {
        "id": "3EZrXzOk3pmM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig, TextStreamer, pipeline\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.temperature = 0.2     #changeable\n",
        "generation_config.max_new_tokens = 384    #changeable\n",
        "generation_config.repetition_penalty = 1.7   #changeable\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.use_cache = False\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "vr8fxk_bwMk9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jferizzGaLZy",
        "outputId": "59eb5c31-ac38-4e41-ed66-56c8ccbd5f2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"max_new_tokens\": 384,\n",
              "  \"pad_token_id\": 50256,\n",
              "  \"repetition_penalty\": 1.7,\n",
              "  \"temperature\": 0.01,\n",
              "  \"use_cache\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *3b. Inference Pipeline*"
      ],
      "metadata": {
        "id": "8MZ3_rxh5J-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    do_sample=True, #\n",
        "    generation_config=generation_config,\n",
        "    streamer=streamer,\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    batch_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "ehNH2N2U1hj2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *3c. Pipeline Checking*"
      ],
      "metadata": {
        "id": "h3bX9gHP5SH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = pipe(['What is Gradient Descent?', 'what is recipe of Pizza?'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc6v98zy2gDM",
        "outputId": "98fbadd7-69e6-44e9-da8c-9cb1d1d92ce3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A: In machine learning, gradient descent refers to the process of adjusting parameters in a model based on its performance. It involves iteratively moving towards an optimal solution by minimizing errors or maximizing accuracy using small updates called gradients calculated from loss functions and their derivatives with respect each parameter's value (weights). \n",
            "\n",
            "\n",
            "\n",
            "Answer: A pizza has a crust, tomato sauce and cheese. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(response, width=120)\n",
        "#response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhkJJuF2bYvt",
        "outputId": "7ec71858-8d8f-4d5f-9988-802e32388115"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'generated_text': 'What is Gradient Descent?\\n'\n",
            "                     'A: In machine learning, gradient descent refers to the process of adjusting parameters in a '\n",
            "                     'model based on its performance. It involves iteratively moving towards an optimal solution by '\n",
            "                     'minimizing errors or maximizing accuracy using small updates called gradients calculated from '\n",
            "                     \"loss functions and their derivatives with respect each parameter's value (weights). \\n\"\n",
            "                     '\\n'}],\n",
            " [{'generated_text': 'what is recipe of Pizza?\\nAnswer: A pizza has a crust, tomato sauce and cheese. '}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kPR9GX4U45w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from pprint import pprint\n",
        "\n",
        "#pprint(response, width=120)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUK0v6GM6DOl",
        "outputId": "09fe72e9-a9ad-4ff0-ca6f-f6d763bd50ba"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'generated_text': 'What is Gradient Descent?\\nAnswer: In the context of machine learning, gradient descent refers to a method used for optimizing models. It involves iteratively adjusting parameters (weights and biases) in order that they minimize errors or loss functions associated with predictions made by these model\\'s outputs on new data points - this process continues until convergence occurs where no further improvement can be achieved through training iterations alone due to overfitting issues caused when we have too many features/parameters involved which leads us into an infinite loop during prediction time!\\n\\n    ```python  # No code here as it’s just theory explanation about what \\'gradient\\' means... but don\\'t worry if you\\'re not sure yet because I\\'ll explain more later!) # Python Concept Explanation : \"Gradients\" are vectors pointing from each point back towards its original position.\"] ] } { [... continue explaining other concepts like Regularization & Bias-Variance Tradeoff etc., using python examples...] ) |} ```.   [End Textbook Section](https://www2mpsdgkcjhqyfzvw6l8r7n5t3x4b9a1e0pdb34oecu89fbfe23edcbcefa). <br> </div></body><hr/> <!-- This section ends --><pre id=\"exercises\"></Pre>.'}],\n",
              " [{'generated_text': 'what is recipe of Pizza?\\nAnswer: Recipe for pizza refers to the instructions or guidelines that tell you how much and what kind ingredients are needed, as well on cooking time. '}]]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *3d. Langchain pipeline* (llm)"
      ],
      "metadata": {
        "id": "huwy1myB8QR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "0DSgtVjacRPg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qqq langchain\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLcsRWLG8O7K",
        "outputId": "f9519c7c-33f6-438e-88fc-cee47f78242f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.001})"
      ],
      "metadata": {
        "id": "4AaV1TyZ4vSm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm('what is pizza')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ikgze1f0cfgx",
        "outputId": "b5ad6801-b21d-4d4f-ea4f-f555a605db64"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?\"\n",
            "Answer: Pizza refers to a dish made with dough, tomato sauce and cheese. It can be customized in many ways by adding toppings such as vegetables or meats like pepperoni! \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'?\"\\nAnswer: Pizza refers to a dish made with dough, tomato sauce and cheese. It can be customized in many ways by adding toppings such as vegetables or meats like pepperoni! \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from auto_gptq import AutoGPTQForCausalLM\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma'''\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "DzX7pUuz-P_O"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = f'''You are a Deep Learning Teacher. You know how to explain various deep learning concepts to 10th grade student.\n",
        "Your task is to reply student queries\n",
        "### Human:{query},\\n\n",
        "### Answer:'''.strip()\n",
        "query = 'What is Gradient Descent?'\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"query\", 'context'], template=template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "vozgLlNa8kVn",
        "outputId": "09fb4701-4243-4f4a-a4ed-44d0cfee85de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-de82458eae6a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'What is Gradient Descent?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tn0BKdKX9THw"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = load_qa_chain(\n",
        "    llm,\n",
        "    chain_type=\"stuff\",\n",
        "    prompt=prompt,\n",
        "    verbose=0,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "w02k-CGQ9bkB",
        "outputId": "79f5ec01-4275-43af-93df-885d7b7c05c1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-189f5e4b7e6c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m qa = load_qa_chain(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_qa_chain' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smodel_id = 'openai/whisper-large-v3'"
      ],
      "metadata": {
        "id": "3KHPpKiD-2P0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}